{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AriSu2904/gist/blob/main/CPU_AD_Global.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psafnzNJ9N8B"
      },
      "source": [
        "#Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqdXUwGe9JuY",
        "outputId": "9750faaf-0cda-4c2e-a17c-d0cb85f6e755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GLOBAL ISOLATION FOREST: MERGE -> TUNE -> EXECUTE\n",
            "============================================================\n",
            "\n",
            "[STEP 1] Merging Data...\n",
            " > Global Train Size : 16122 rows\n",
            " > Global Test Size  : 16128 rows\n",
            "\n",
            "[STEP 2] Preprocessing...\n",
            "\n",
            "[STEP 3] Tuning Global Parameters...\n",
            "n_est      | max_samp   | AUC (Avg)  | Time (s)  \n",
            "--------------------------------------------------\n",
            "100        | 128        | 0.7073     | 0.6375\n",
            "100        | 256        | 0.7109     | 0.6118\n",
            "100        | 512        | 0.7134     | 0.8181\n",
            "200        | 128        | 0.7055     | 1.9802\n",
            "200        | 256        | 0.7125     | 1.7021\n",
            "200        | 512        | 0.7073     | 1.4439\n",
            "300        | 128        | 0.7047     | 1.7676\n",
            "300        | 256        | 0.7107     | 1.5556\n",
            "300        | 512        | 0.7096     | 1.1003\n",
            "--------------------------------------------------\n",
            "JUARA GLOBAL: n_estimators=100, max_samples=512\n",
            "Skor AUC Global Sementara: 0.7134\n",
            "\n",
            "[STEP 4] Final Evaluation Per Server (Using Best Params)...\n",
            "-----------------------------------------------------------------\n",
            "Source                         | AUC        | F1        \n",
            "-----------------------------------------------------------------\n",
            "ec2_cpu_utilization_24ae8d     | 0.9935     | 0.2857\n",
            "ec2_cpu_utilization_53ea38     | 0.7164     | 0.0032\n",
            "ec2_cpu_utilization_5f5533     | 0.8888     | 0.0088\n",
            "ec2_cpu_utilization_ac20cd     | 0.9995     | 0.6667\n",
            "ec2_cpu_utilization_fe7f93     | 0.7997     | 0.0278\n",
            "=================================================================\n",
            "FINAL RESULT: GLOBAL ISOLATION FOREST\n",
            "=================================================================\n",
            "                       source       auc        f1\n",
            "0  ec2_cpu_utilization_24ae8d  0.993545  0.285714\n",
            "1  ec2_cpu_utilization_53ea38  0.716377  0.003165\n",
            "2  ec2_cpu_utilization_5f5533  0.888834  0.008772\n",
            "3  ec2_cpu_utilization_ac20cd  0.999504  0.666667\n",
            "4  ec2_cpu_utilization_fe7f93  0.799652  0.027778\n",
            "-----------------------------------------------------------------\n",
            "AVERAGE AUC (Macro): 0.8796\n",
            "=================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "# ==========================================\n",
        "# --- CONFIGURATION ---\n",
        "# ==========================================\n",
        "INPUT_FOLDER = '/content/drive/MyDrive/NAB_RESOURCES/nab_resources/nab_final_split'\n",
        "\n",
        "# GRID PARAMETER (Range lebih besar karena data lebih besar)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_samples': [128, 256, 512]   # Kita naikkan opsinya karena data global lebih variatif\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GLOBAL ISOLATION FOREST: MERGE -> TUNE -> EXECUTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: MERGE DATA (MEMBUAT DATASET RAKSASA)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 1] Merging Data...\")\n",
        "train_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_train.csv\")))\n",
        "test_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_test.csv\")))\n",
        "\n",
        "if not train_files:\n",
        "    print(\"[ERROR] File training tidak ditemukan.\")\n",
        "    exit()\n",
        "\n",
        "# Gabung Training\n",
        "train_list = []\n",
        "for f in train_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_train.csv', '')\n",
        "    train_list.append(df)\n",
        "global_train = pd.concat(train_list, ignore_index=True)\n",
        "\n",
        "# Gabung Testing\n",
        "test_list = []\n",
        "for f in test_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_test.csv', '')\n",
        "    test_list.append(df)\n",
        "global_test = pd.concat(test_list, ignore_index=True)\n",
        "\n",
        "print(f\" > Global Train Size : {len(global_train)} rows\")\n",
        "print(f\" > Global Test Size  : {len(global_test)} rows\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPARATION & SCALING\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 2] Preprocessing...\")\n",
        "scaler = MinMaxScaler()\n",
        "# Fit di Global Train\n",
        "global_train['value_scaled'] = scaler.fit_transform(global_train['value'].values.reshape(-1,1))\n",
        "# Transform di Global Test\n",
        "global_test['value_scaled'] = scaler.transform(global_test['value'].values.reshape(-1,1))\n",
        "\n",
        "# Feature Engineering\n",
        "for df in [global_train, global_test]:\n",
        "    df['roll_mean'] = df['value_scaled'].rolling(5, min_periods=1).mean()\n",
        "    df['diff'] = df['value_scaled'].diff().fillna(0)\n",
        "    df['accel'] = df['diff'].diff().fillna(0)\n",
        "\n",
        "feats = ['value_scaled', 'roll_mean', 'diff', 'accel']\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: HYPERPARAMETER TUNING (KHUSUS GLOBAL)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 3] Tuning Global Parameters...\")\n",
        "print(f\"{'n_est':<10} | {'max_samp':<10} | {'AUC (Avg)':<10} | {'Time (s)':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "best_auc = -1\n",
        "best_params = {}\n",
        "\n",
        "keys = param_grid.keys()\n",
        "combinations = list(itertools.product(*param_grid.values()))\n",
        "\n",
        "# Kita tuning berdasarkan performa rata-rata di Global Test\n",
        "for combo in combinations:\n",
        "    params = dict(zip(keys, combo))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Train Model Global\n",
        "    model = IsolationForest(\n",
        "        n_estimators=params['n_estimators'],\n",
        "        max_samples=params['max_samples'],\n",
        "        contamination='auto',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model.fit(global_train[feats])\n",
        "\n",
        "    # Test ke semua data test\n",
        "    scores = -model.score_samples(global_test[feats])\n",
        "\n",
        "    # Hitung AUC Global (Micro Average) - Cara cepat buat tuning\n",
        "    # (Di step final nanti kita hitung per server biar detail)\n",
        "    try:\n",
        "        auc = roc_auc_score(global_test['label'], scores)\n",
        "    except:\n",
        "        auc = 0\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"{params['n_estimators']:<10} | {params['max_samples']:<10} | {auc:.4f}     | {dt:.4f}\")\n",
        "\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_params = params\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"JUARA GLOBAL: n_estimators={best_params['n_estimators']}, max_samples={best_params['max_samples']}\")\n",
        "print(f\"Skor AUC Global Sementara: {best_auc:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 4: FINAL EXECUTION & EVALUATION PER SOURCE\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 4] Final Evaluation Per Server (Using Best Params)...\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Source':<30} | {'AUC':<10} | {'F1':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# Train Final Model\n",
        "final_model = IsolationForest(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_samples=best_params['max_samples'],\n",
        "    contamination='auto',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "final_model.fit(global_train[feats])\n",
        "\n",
        "# Evaluasi per server (biar bisa dibandingin sama Local Model)\n",
        "unique_sources = global_test['source'].unique()\n",
        "results = []\n",
        "\n",
        "for source in unique_sources:\n",
        "    # Ambil data khusus server ini\n",
        "    subset = global_test[global_test['source'] == source].copy()\n",
        "\n",
        "    if len(subset['label'].unique()) > 1:\n",
        "        scores = -final_model.score_samples(subset[feats])\n",
        "        auc = roc_auc_score(subset['label'], scores)\n",
        "\n",
        "        # Cari F1 Terbaik\n",
        "        best_f1 = 0\n",
        "        threshs = np.linspace(scores.min(), scores.max(), 100)\n",
        "        for t in threshs:\n",
        "            p = (scores > t).astype(int)\n",
        "            f = f1_score(subset['label'], p, zero_division=0)\n",
        "            if f > best_f1: best_f1 = f\n",
        "\n",
        "        print(f\"{source:<30} | {auc:.4f}     | {best_f1:.4f}\")\n",
        "        results.append({'source': source, 'auc': auc, 'f1': best_f1})\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# SUMMARY\n",
        "# ---------------------------------------------------------\n",
        "print(\"=\"*65)\n",
        "print(\"FINAL RESULT: GLOBAL ISOLATION FOREST\")\n",
        "print(\"=\"*65)\n",
        "df_res = pd.DataFrame(results)\n",
        "if not df_res.empty:\n",
        "    print(df_res)\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"AVERAGE AUC (Macro): {df_res['auc'].mean():.4f}\")\n",
        "else:\n",
        "    print(\"No valid results.\")\n",
        "print(\"=\"*65)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "# ==========================================\n",
        "# --- CONFIGURATION ---\n",
        "# ==========================================\n",
        "INPUT_FOLDER = '/content/drive/MyDrive/NAB_RESOURCES/nab_resources/nab_final_split_30'\n",
        "\n",
        "# GRID PARAMETER (Range lebih besar karena data lebih besar)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_samples': [128, 256, 512]   # Kita naikkan opsinya karena data global lebih variatif\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GLOBAL ISOLATION FOREST: MERGE -> TUNE -> EXECUTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: MERGE DATA (MEMBUAT DATASET RAKSASA)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 1] Merging Data...\")\n",
        "train_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_train.csv\")))\n",
        "test_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_test.csv\")))\n",
        "\n",
        "if not train_files:\n",
        "    print(\"[ERROR] File training tidak ditemukan.\")\n",
        "    exit()\n",
        "\n",
        "# Gabung Training\n",
        "train_list = []\n",
        "for f in train_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_train.csv', '')\n",
        "    train_list.append(df)\n",
        "global_train = pd.concat(train_list, ignore_index=True)\n",
        "\n",
        "# Gabung Testing\n",
        "test_list = []\n",
        "for f in test_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_test.csv', '')\n",
        "    test_list.append(df)\n",
        "global_test = pd.concat(test_list, ignore_index=True)\n",
        "\n",
        "print(f\" > Global Train Size : {len(global_train)} rows\")\n",
        "print(f\" > Global Test Size  : {len(global_test)} rows\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPARATION & SCALING\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 2] Preprocessing...\")\n",
        "scaler = MinMaxScaler()\n",
        "# Fit di Global Train\n",
        "global_train['value_scaled'] = scaler.fit_transform(global_train['value'].values.reshape(-1,1))\n",
        "# Transform di Global Test\n",
        "global_test['value_scaled'] = scaler.transform(global_test['value'].values.reshape(-1,1))\n",
        "\n",
        "# Feature Engineering\n",
        "for df in [global_train, global_test]:\n",
        "    df['roll_mean'] = df['value_scaled'].rolling(5, min_periods=1).mean()\n",
        "    df['diff'] = df['value_scaled'].diff().fillna(0)\n",
        "    df['accel'] = df['diff'].diff().fillna(0)\n",
        "\n",
        "feats = ['value_scaled', 'roll_mean', 'diff', 'accel']\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: HYPERPARAMETER TUNING (KHUSUS GLOBAL)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 3] Tuning Global Parameters...\")\n",
        "print(f\"{'n_est':<10} | {'max_samp':<10} | {'AUC (Avg)':<10} | {'Time (s)':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "best_auc = -1\n",
        "best_params = {}\n",
        "\n",
        "keys = param_grid.keys()\n",
        "combinations = list(itertools.product(*param_grid.values()))\n",
        "\n",
        "# Kita tuning berdasarkan performa rata-rata di Global Test\n",
        "for combo in combinations:\n",
        "    params = dict(zip(keys, combo))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Train Model Global\n",
        "    model = IsolationForest(\n",
        "        n_estimators=params['n_estimators'],\n",
        "        max_samples=params['max_samples'],\n",
        "        contamination='auto',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model.fit(global_train[feats])\n",
        "\n",
        "    # Test ke semua data test\n",
        "    scores = -model.score_samples(global_test[feats])\n",
        "\n",
        "    # Hitung AUC Global (Micro Average) - Cara cepat buat tuning\n",
        "    # (Di step final nanti kita hitung per server biar detail)\n",
        "    try:\n",
        "        auc = roc_auc_score(global_test['label'], scores)\n",
        "    except:\n",
        "        auc = 0\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"{params['n_estimators']:<10} | {params['max_samples']:<10} | {auc:.4f}     | {dt:.4f}\")\n",
        "\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_params = params\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"JUARA GLOBAL: n_estimators={best_params['n_estimators']}, max_samples={best_params['max_samples']}\")\n",
        "print(f\"Skor AUC Global Sementara: {best_auc:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 4: FINAL EXECUTION & EVALUATION PER SOURCE\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 4] Final Evaluation Per Server (Using Best Params)...\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Source':<30} | {'AUC':<10} | {'F1':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# Train Final Model\n",
        "final_model = IsolationForest(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_samples=best_params['max_samples'],\n",
        "    contamination='auto',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "final_model.fit(global_train[feats])\n",
        "\n",
        "# Evaluasi per server (biar bisa dibandingin sama Local Model)\n",
        "unique_sources = global_test['source'].unique()\n",
        "results = []\n",
        "\n",
        "for source in unique_sources:\n",
        "    # Ambil data khusus server ini\n",
        "    subset = global_test[global_test['source'] == source].copy()\n",
        "\n",
        "    if len(subset['label'].unique()) > 1:\n",
        "        scores = -final_model.score_samples(subset[feats])\n",
        "        auc = roc_auc_score(subset['label'], scores)\n",
        "\n",
        "        # Cari F1 Terbaik\n",
        "        best_f1 = 0\n",
        "        threshs = np.linspace(scores.min(), scores.max(), 100)\n",
        "        for t in threshs:\n",
        "            p = (scores > t).astype(int)\n",
        "            f = f1_score(subset['label'], p, zero_division=0)\n",
        "            if f > best_f1: best_f1 = f\n",
        "\n",
        "        print(f\"{source:<30} | {auc:.4f}     | {best_f1:.4f}\")\n",
        "        results.append({'source': source, 'auc': auc, 'f1': best_f1})\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# SUMMARY\n",
        "# ---------------------------------------------------------\n",
        "print(\"=\"*65)\n",
        "print(\"FINAL RESULT: GLOBAL ISOLATION FOREST\")\n",
        "print(\"=\"*65)\n",
        "df_res = pd.DataFrame(results)\n",
        "if not df_res.empty:\n",
        "    print(df_res)\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"AVERAGE AUC (Macro): {df_res['auc'].mean():.4f}\")\n",
        "else:\n",
        "    print(\"No valid results.\")\n",
        "print(\"=\"*65)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulq6tTfE59ik",
        "outputId": "d68baa19-0e5a-4207-cd6d-e63397c866b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GLOBAL ISOLATION FOREST: MERGE -> TUNE -> EXECUTE\n",
            "============================================================\n",
            "\n",
            "[STEP 1] Merging Data...\n",
            " > Global Train Size : 9671 rows\n",
            " > Global Test Size  : 22584 rows\n",
            "\n",
            "[STEP 2] Preprocessing...\n",
            "\n",
            "[STEP 3] Tuning Global Parameters...\n",
            "n_est      | max_samp   | AUC (Avg)  | Time (s)  \n",
            "--------------------------------------------------\n",
            "100        | 128        | 0.7655     | 0.3778\n",
            "100        | 256        | 0.7750     | 0.4020\n",
            "100        | 512        | 0.7721     | 0.3990\n",
            "200        | 128        | 0.7627     | 0.7361\n",
            "200        | 256        | 0.7703     | 0.7816\n",
            "200        | 512        | 0.7711     | 1.0181\n",
            "300        | 128        | 0.7572     | 1.3837\n",
            "300        | 256        | 0.7683     | 1.4287\n",
            "300        | 512        | 0.7730     | 1.1434\n",
            "--------------------------------------------------\n",
            "JUARA GLOBAL: n_estimators=100, max_samples=256\n",
            "Skor AUC Global Sementara: 0.7750\n",
            "\n",
            "[STEP 4] Final Evaluation Per Server (Using Best Params)...\n",
            "-----------------------------------------------------------------\n",
            "Source                         | AUC        | F1        \n",
            "-----------------------------------------------------------------\n",
            "ec2_cpu_utilization_24ae8d     | 0.9959     | 0.5000\n",
            "ec2_cpu_utilization_53ea38     | 0.8865     | 0.0308\n",
            "ec2_cpu_utilization_5f5533     | 0.9617     | 0.0625\n",
            "ec2_cpu_utilization_77c1ca     | 0.7927     | 0.0034\n",
            "ec2_cpu_utilization_825cc2     | 0.9963     | 0.2353\n",
            "ec2_cpu_utilization_ac20cd     | 0.9996     | 0.6667\n",
            "ec2_cpu_utilization_fe7f93     | 0.7621     | 0.0171\n",
            "=================================================================\n",
            "FINAL RESULT: GLOBAL ISOLATION FOREST\n",
            "=================================================================\n",
            "                       source       auc        f1\n",
            "0  ec2_cpu_utilization_24ae8d  0.995923  0.500000\n",
            "1  ec2_cpu_utilization_53ea38  0.886476  0.030769\n",
            "2  ec2_cpu_utilization_5f5533  0.961716  0.062500\n",
            "3  ec2_cpu_utilization_77c1ca  0.792700  0.003390\n",
            "4  ec2_cpu_utilization_825cc2  0.996278  0.235294\n",
            "5  ec2_cpu_utilization_ac20cd  0.999646  0.666667\n",
            "6  ec2_cpu_utilization_fe7f93  0.762141  0.017094\n",
            "-----------------------------------------------------------------\n",
            "AVERAGE AUC (Macro): 0.9136\n",
            "=================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-qJz-eM-EKf"
      },
      "source": [
        "#VAE-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2SO5_5J-Fb6",
        "outputId": "81eda559-9c3a-493c-cc64-56a57492678e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GLOBAL VAE-LSTM: MERGE -> TUNE -> EXECUTE (ULTIMATE)\n",
            "============================================================\n",
            "\n",
            "[STEP 1] Merging Data...\n",
            " > Global Train Size : 16122 rows\n",
            " > Global Test Size  : 16128 rows\n",
            "\n",
            "[STEP 2] Preprocessing...\n",
            "\n",
            "[STEP 4] Tuning Global Parameters...\n",
            "latent     | lstm       | AUC (Global) | Time (s)  \n",
            "-------------------------------------------------------\n",
            "4          | 32         | 0.1431       | 49.57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4          | 64         | nan       | 73.03\n",
            "8          | 32         | 0.4303       | 38.79\n",
            "8          | 64         | 0.2460       | 70.99\n",
            "-------------------------------------------------------\n",
            "JUARA GLOBAL: latent=8, lstm=32\n",
            "\n",
            "[STEP 5] Final Evaluation Per Server...\n",
            "-----------------------------------------------------------------\n",
            "Source                         | AUC        | F1        \n",
            "-----------------------------------------------------------------\n",
            "ec2_cpu_utilization_24ae8d     | 0.4838     | 0.0020\n",
            "ec2_cpu_utilization_53ea38     | 0.3825     | 0.0010\n",
            "ec2_cpu_utilization_5f5533     | 0.7716     | 0.0031\n",
            "ec2_cpu_utilization_ac20cd     | 0.9736     | 0.0312\n",
            "ec2_cpu_utilization_fe7f93     | 0.6163     | 0.4000\n",
            "=================================================================\n",
            "FINAL RESULT: GLOBAL VAE-LSTM (TUNED)\n",
            "=================================================================\n",
            "                       source       auc        f1\n",
            "0  ec2_cpu_utilization_24ae8d  0.483782  0.001993\n",
            "1  ec2_cpu_utilization_53ea38  0.382544  0.000997\n",
            "2  ec2_cpu_utilization_5f5533  0.771571  0.003077\n",
            "3  ec2_cpu_utilization_ac20cd  0.973566  0.031250\n",
            "4  ec2_cpu_utilization_fe7f93  0.616267  0.400000\n",
            "-----------------------------------------------------------------\n",
            "AVERAGE AUC (Macro): 0.6455\n",
            "=================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "# ==========================================\n",
        "# --- CONFIGURATION ---\n",
        "# ==========================================\n",
        "INPUT_FOLDER = '/content/drive/MyDrive/NAB_RESOURCES/nab_resources/nab_final_split'\n",
        "TIME_STEPS = 10\n",
        "\n",
        "# GRID PARAMETER UNTUK TUNING\n",
        "param_grid = {\n",
        "    'latent_dim': [4, 8],       # Coba naikin dikit siapa tau butuh info lebih\n",
        "    'lstm_units': [32, 64]      # Coba naikin kapasitas otak\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GLOBAL VAE-LSTM: MERGE -> TUNE -> EXECUTE (ULTIMATE)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: MERGE DATA\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 1] Merging Data...\")\n",
        "train_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_train.csv\")))\n",
        "test_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_test.csv\")))\n",
        "\n",
        "train_list = []\n",
        "for f in train_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_train.csv', '')\n",
        "    train_list.append(df)\n",
        "global_train = pd.concat(train_list, ignore_index=True)\n",
        "\n",
        "test_list = []\n",
        "for f in test_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_test.csv', '')\n",
        "    test_list.append(df)\n",
        "global_test = pd.concat(test_list, ignore_index=True)\n",
        "\n",
        "print(f\" > Global Train Size : {len(global_train)} rows\")\n",
        "print(f\" > Global Test Size  : {len(global_test)} rows\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPROCESSING\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 2] Preprocessing...\")\n",
        "scaler = StandardScaler()\n",
        "global_train_vals = scaler.fit_transform(global_train['value'].values.reshape(-1,1))\n",
        "global_test['value_scaled'] = scaler.transform(global_test['value'].values.reshape(-1,1))\n",
        "\n",
        "def create_sequences(values):\n",
        "    xs = []\n",
        "    if len(values) > TIME_STEPS:\n",
        "        for i in range(len(values) - TIME_STEPS):\n",
        "            xs.append(values[i:(i + TIME_STEPS)])\n",
        "    return np.array(xs)\n",
        "\n",
        "X_train_global = create_sequences(global_train_vals)\n",
        "# Ambil label global (potong 10 di awal karena sequence)\n",
        "# Note: Label global ini cuma buat tuning cepat, nanti final eval tetep per source\n",
        "y_test_global = global_test['label'].values[TIME_STEPS:]\n",
        "X_test_global = create_sequences(global_test['value_scaled'].values.reshape(-1,1))\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: MODEL BUILDER (Robust Version)\n",
        "# ---------------------------------------------------------\n",
        "def build_vae_dynamic(lstm_units, latent_dim):\n",
        "    enc_in = keras.Input(shape=(TIME_STEPS, 1))\n",
        "    x = layers.LSTM(lstm_units, return_sequences=False)(enc_in)\n",
        "    z_mean = layers.Dense(latent_dim)(x)\n",
        "    z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "    def sampling(args):\n",
        "        zm, zv = args\n",
        "        batch = tf.shape(zm)[0]\n",
        "        dim = tf.shape(zm)[1]\n",
        "        eps = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return zm + tf.keras.backend.exp(0.5 * zv) * eps\n",
        "\n",
        "    z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "    encoder = keras.Model(enc_in, [z_mean, z_log_var, z])\n",
        "\n",
        "    dec_in = keras.Input(shape=(latent_dim,))\n",
        "    x = layers.RepeatVector(TIME_STEPS)(dec_in)\n",
        "    x = layers.LSTM(lstm_units, return_sequences=True)(x)\n",
        "    dec_out = layers.TimeDistributed(layers.Dense(1))(x)\n",
        "    decoder = keras.Model(dec_in, dec_out)\n",
        "\n",
        "    class VAE(keras.Model):\n",
        "        def __init__(self, enc, dec):\n",
        "            super().__init__()\n",
        "            self.enc = enc\n",
        "            self.dec = dec\n",
        "        def train_step(self, data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                zm, zv, z = self.enc(data)\n",
        "                recon = self.dec(z)\n",
        "                recon_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(data, recon), axis=1))\n",
        "                kl_loss = -0.5 * (1 + zv - tf.square(zm) - tf.exp(zv))\n",
        "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                total_loss = recon_loss + kl_loss\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "            return {\"loss\": total_loss}\n",
        "        def call(self, inputs):\n",
        "            zm, _, z = self.enc(inputs)\n",
        "            return self.dec(z)\n",
        "\n",
        "    vae = VAE(encoder, decoder)\n",
        "    vae.compile(optimizer='adam')\n",
        "    return vae\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 4: TUNING LOOP\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 4] Tuning Global Parameters...\")\n",
        "print(f\"{'latent':<10} | {'lstm':<10} | {'AUC (Global)':<12} | {'Time (s)':<10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "keys = param_grid.keys()\n",
        "combinations = list(itertools.product(*param_grid.values()))\n",
        "\n",
        "best_auc = -1\n",
        "best_params = {}\n",
        "\n",
        "for combo in combinations:\n",
        "    params = dict(zip(keys, combo))\n",
        "    tf.keras.backend.clear_session() # Bersihin memori\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    vae = build_vae_dynamic(params['lstm_units'], params['latent_dim'])\n",
        "    # Epoch dikit aja (15) buat screening\n",
        "    vae.fit(X_train_global, epochs=15, batch_size=128, verbose=0)\n",
        "\n",
        "    # Predict Global (Quick Check)\n",
        "    # Kita sample sebagian test set aja biar cepet kalau kegedean\n",
        "    idx = np.random.choice(len(X_test_global), size=min(5000, len(X_test_global)), replace=False)\n",
        "    pred = vae.predict(X_test_global[idx], verbose=0)\n",
        "    scores = np.mean(np.mean(np.square(X_test_global[idx] - pred), axis=1), axis=1)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test_global[idx], scores)\n",
        "    except:\n",
        "        auc = 0.5\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"{params['latent_dim']:<10} | {params['lstm_units']:<10} | {auc:.4f}       | {dt:.2f}\")\n",
        "\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_params = params\n",
        "\n",
        "print(\"-\" * 55)\n",
        "print(f\"JUARA GLOBAL: latent={best_params['latent_dim']}, lstm={best_params['lstm_units']}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 5: FINAL EXECUTION (WITH BEST PARAMS)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 5] Final Evaluation Per Server...\")\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Re-Train Full Power (30 Epochs)\n",
        "final_vae = build_vae_dynamic(best_params['lstm_units'], best_params['latent_dim'])\n",
        "final_vae.fit(X_train_global, epochs=30, batch_size=64, verbose=0)\n",
        "\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Source':<30} | {'AUC':<10} | {'F1':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "unique_sources = global_test['source'].unique()\n",
        "results = []\n",
        "\n",
        "for source in unique_sources:\n",
        "    subset = global_test[global_test['source'] == source].copy()\n",
        "\n",
        "    if len(subset) > TIME_STEPS + 5:\n",
        "        sub_vals = subset['value_scaled'].values.reshape(-1,1)\n",
        "        X_sub = create_sequences(sub_vals)\n",
        "        y_sub = subset['label'].values[TIME_STEPS:]\n",
        "\n",
        "        if len(np.unique(y_sub)) > 1:\n",
        "            pred = final_vae.predict(X_sub, verbose=0)\n",
        "            scores = np.mean(np.mean(np.square(X_sub - pred), axis=1), axis=1)\n",
        "\n",
        "            auc = roc_auc_score(y_sub, scores)\n",
        "\n",
        "            best_f1 = 0\n",
        "            threshs = np.linspace(scores.min(), scores.max(), 100)\n",
        "            for t in threshs:\n",
        "                p = (scores > t).astype(int)\n",
        "                f = f1_score(y_sub, p, zero_division=0)\n",
        "                if f > best_f1: best_f1 = f\n",
        "\n",
        "            print(f\"{source:<30} | {auc:.4f}     | {best_f1:.4f}\")\n",
        "            results.append({'source': source, 'auc': auc, 'f1': best_f1})\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"FINAL RESULT: GLOBAL VAE-LSTM (TUNED)\")\n",
        "print(\"=\"*65)\n",
        "df_res = pd.DataFrame(results)\n",
        "if not df_res.empty:\n",
        "    print(df_res)\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"AVERAGE AUC (Macro): {df_res['auc'].mean():.4f}\")\n",
        "else:\n",
        "    print(\"No valid results.\")\n",
        "print(\"=\"*65)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "# ==========================================\n",
        "# --- CONFIGURATION ---\n",
        "# ==========================================\n",
        "INPUT_FOLDER = '/content/drive/MyDrive/NAB_RESOURCES/nab_resources/nab_final_split_30'\n",
        "TIME_STEPS = 10\n",
        "\n",
        "# GRID PARAMETER UNTUK TUNING\n",
        "param_grid = {\n",
        "    'latent_dim': [4, 8],       # Coba naikin dikit siapa tau butuh info lebih\n",
        "    'lstm_units': [32, 64]      # Coba naikin kapasitas otak\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GLOBAL VAE-LSTM: MERGE -> TUNE -> EXECUTE (ULTIMATE)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: MERGE DATA\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 1] Merging Data...\")\n",
        "train_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_train.csv\")))\n",
        "test_files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*_test.csv\")))\n",
        "\n",
        "train_list = []\n",
        "for f in train_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_train.csv', '')\n",
        "    train_list.append(df)\n",
        "global_train = pd.concat(train_list, ignore_index=True)\n",
        "\n",
        "test_list = []\n",
        "for f in test_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['source'] = os.path.basename(f).replace('_test.csv', '')\n",
        "    test_list.append(df)\n",
        "global_test = pd.concat(test_list, ignore_index=True)\n",
        "\n",
        "print(f\" > Global Train Size : {len(global_train)} rows\")\n",
        "print(f\" > Global Test Size  : {len(global_test)} rows\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPROCESSING\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 2] Preprocessing...\")\n",
        "scaler = StandardScaler()\n",
        "global_train_vals = scaler.fit_transform(global_train['value'].values.reshape(-1,1))\n",
        "global_test['value_scaled'] = scaler.transform(global_test['value'].values.reshape(-1,1))\n",
        "\n",
        "def create_sequences(values):\n",
        "    xs = []\n",
        "    if len(values) > TIME_STEPS:\n",
        "        for i in range(len(values) - TIME_STEPS):\n",
        "            xs.append(values[i:(i + TIME_STEPS)])\n",
        "    return np.array(xs)\n",
        "\n",
        "X_train_global = create_sequences(global_train_vals)\n",
        "# Ambil label global (potong 10 di awal karena sequence)\n",
        "# Note: Label global ini cuma buat tuning cepat, nanti final eval tetep per source\n",
        "y_test_global = global_test['label'].values[TIME_STEPS:]\n",
        "X_test_global = create_sequences(global_test['value_scaled'].values.reshape(-1,1))\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: MODEL BUILDER (Robust Version)\n",
        "# ---------------------------------------------------------\n",
        "def build_vae_dynamic(lstm_units, latent_dim):\n",
        "    enc_in = keras.Input(shape=(TIME_STEPS, 1))\n",
        "    x = layers.LSTM(lstm_units, return_sequences=False)(enc_in)\n",
        "    z_mean = layers.Dense(latent_dim)(x)\n",
        "    z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "    def sampling(args):\n",
        "        zm, zv = args\n",
        "        batch = tf.shape(zm)[0]\n",
        "        dim = tf.shape(zm)[1]\n",
        "        eps = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return zm + tf.keras.backend.exp(0.5 * zv) * eps\n",
        "\n",
        "    z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "    encoder = keras.Model(enc_in, [z_mean, z_log_var, z])\n",
        "\n",
        "    dec_in = keras.Input(shape=(latent_dim,))\n",
        "    x = layers.RepeatVector(TIME_STEPS)(dec_in)\n",
        "    x = layers.LSTM(lstm_units, return_sequences=True)(x)\n",
        "    dec_out = layers.TimeDistributed(layers.Dense(1))(x)\n",
        "    decoder = keras.Model(dec_in, dec_out)\n",
        "\n",
        "    class VAE(keras.Model):\n",
        "        def __init__(self, enc, dec):\n",
        "            super().__init__()\n",
        "            self.enc = enc\n",
        "            self.dec = dec\n",
        "        def train_step(self, data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                zm, zv, z = self.enc(data)\n",
        "                recon = self.dec(z)\n",
        "                recon_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(data, recon), axis=1))\n",
        "                kl_loss = -0.5 * (1 + zv - tf.square(zm) - tf.exp(zv))\n",
        "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                total_loss = recon_loss + kl_loss\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "            return {\"loss\": total_loss}\n",
        "        def call(self, inputs):\n",
        "            zm, _, z = self.enc(inputs)\n",
        "            return self.dec(z)\n",
        "\n",
        "    vae = VAE(encoder, decoder)\n",
        "    vae.compile(optimizer='adam')\n",
        "    return vae\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 4: TUNING LOOP\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 4] Tuning Global Parameters...\")\n",
        "print(f\"{'latent':<10} | {'lstm':<10} | {'AUC (Global)':<12} | {'Time (s)':<10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "keys = param_grid.keys()\n",
        "combinations = list(itertools.product(*param_grid.values()))\n",
        "\n",
        "best_auc = -1\n",
        "best_params = {}\n",
        "\n",
        "for combo in combinations:\n",
        "    params = dict(zip(keys, combo))\n",
        "    tf.keras.backend.clear_session() # Bersihin memori\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    vae = build_vae_dynamic(params['lstm_units'], params['latent_dim'])\n",
        "    # Epoch dikit aja (15) buat screening\n",
        "    vae.fit(X_train_global, epochs=15, batch_size=128, verbose=0)\n",
        "\n",
        "    # Predict Global (Quick Check)\n",
        "    # Kita sample sebagian test set aja biar cepet kalau kegedean\n",
        "    idx = np.random.choice(len(X_test_global), size=min(5000, len(X_test_global)), replace=False)\n",
        "    pred = vae.predict(X_test_global[idx], verbose=0)\n",
        "    scores = np.mean(np.mean(np.square(X_test_global[idx] - pred), axis=1), axis=1)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test_global[idx], scores)\n",
        "    except:\n",
        "        auc = 0.5\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"{params['latent_dim']:<10} | {params['lstm_units']:<10} | {auc:.4f}       | {dt:.2f}\")\n",
        "\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_params = params\n",
        "\n",
        "print(\"-\" * 55)\n",
        "print(f\"JUARA GLOBAL: latent={best_params['latent_dim']}, lstm={best_params['lstm_units']}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 5: FINAL EXECUTION (WITH BEST PARAMS)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n[STEP 5] Final Evaluation Per Server...\")\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Re-Train Full Power (30 Epochs)\n",
        "final_vae = build_vae_dynamic(best_params['lstm_units'], best_params['latent_dim'])\n",
        "final_vae.fit(X_train_global, epochs=30, batch_size=64, verbose=0)\n",
        "\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Source':<30} | {'AUC':<10} | {'F1':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "unique_sources = global_test['source'].unique()\n",
        "results = []\n",
        "\n",
        "for source in unique_sources:\n",
        "    subset = global_test[global_test['source'] == source].copy()\n",
        "\n",
        "    if len(subset) > TIME_STEPS + 5:\n",
        "        sub_vals = subset['value_scaled'].values.reshape(-1,1)\n",
        "        X_sub = create_sequences(sub_vals)\n",
        "        y_sub = subset['label'].values[TIME_STEPS:]\n",
        "\n",
        "        if len(np.unique(y_sub)) > 1:\n",
        "            pred = final_vae.predict(X_sub, verbose=0)\n",
        "            scores = np.mean(np.mean(np.square(X_sub - pred), axis=1), axis=1)\n",
        "\n",
        "            auc = roc_auc_score(y_sub, scores)\n",
        "\n",
        "            best_f1 = 0\n",
        "            threshs = np.linspace(scores.min(), scores.max(), 100)\n",
        "            for t in threshs:\n",
        "                p = (scores > t).astype(int)\n",
        "                f = f1_score(y_sub, p, zero_division=0)\n",
        "                if f > best_f1: best_f1 = f\n",
        "\n",
        "            print(f\"{source:<30} | {auc:.4f}     | {best_f1:.4f}\")\n",
        "            results.append({'source': source, 'auc': auc, 'f1': best_f1})\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"FINAL RESULT: GLOBAL VAE-LSTM (TUNED)\")\n",
        "print(\"=\"*65)\n",
        "df_res = pd.DataFrame(results)\n",
        "if not df_res.empty:\n",
        "    print(df_res)\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"AVERAGE AUC (Macro): {df_res['auc'].mean():.4f}\")\n",
        "else:\n",
        "    print(\"No valid results.\")\n",
        "print(\"=\"*65)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX_xu0uN7Eea",
        "outputId": "73db7db9-3933-497e-aa84-6cf5682972c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GLOBAL VAE-LSTM: MERGE -> TUNE -> EXECUTE (ULTIMATE)\n",
            "============================================================\n",
            "\n",
            "[STEP 1] Merging Data...\n",
            " > Global Train Size : 9671 rows\n",
            " > Global Test Size  : 22584 rows\n",
            "\n",
            "[STEP 2] Preprocessing...\n",
            "\n",
            "[STEP 4] Tuning Global Parameters...\n",
            "latent     | lstm       | AUC (Global) | Time (s)  \n",
            "-------------------------------------------------------\n",
            "4          | 32         | 0.1191       | 25.05\n",
            "4          | 64         | 0.4397       | 43.67\n",
            "8          | 32         | 0.3162       | 25.10\n",
            "8          | 64         | 0.6450       | 42.06\n",
            "-------------------------------------------------------\n",
            "JUARA GLOBAL: latent=8, lstm=64\n",
            "\n",
            "[STEP 5] Final Evaluation Per Server...\n",
            "-----------------------------------------------------------------\n",
            "Source                         | AUC        | F1        \n",
            "-----------------------------------------------------------------\n",
            "ec2_cpu_utilization_24ae8d     | 0.5229     | 0.0150\n",
            "ec2_cpu_utilization_53ea38     | 0.6757     | 0.0014\n",
            "ec2_cpu_utilization_5f5533     | 0.2921     | 0.0014\n",
            "ec2_cpu_utilization_77c1ca     | 0.6202     | 0.0018\n",
            "ec2_cpu_utilization_825cc2     | 0.6537     | 0.0192\n",
            "ec2_cpu_utilization_ac20cd     | 0.8748     | 0.0046\n",
            "ec2_cpu_utilization_fe7f93     | 0.5982     | 0.0075\n",
            "=================================================================\n",
            "FINAL RESULT: GLOBAL VAE-LSTM (TUNED)\n",
            "=================================================================\n",
            "                       source       auc        f1\n",
            "0  ec2_cpu_utilization_24ae8d  0.522946  0.015038\n",
            "1  ec2_cpu_utilization_53ea38  0.675738  0.001421\n",
            "2  ec2_cpu_utilization_5f5533  0.292067  0.001421\n",
            "3  ec2_cpu_utilization_77c1ca  0.620199  0.001764\n",
            "4  ec2_cpu_utilization_825cc2  0.653682  0.019231\n",
            "5  ec2_cpu_utilization_ac20cd  0.874822  0.004619\n",
            "6  ec2_cpu_utilization_fe7f93  0.598186  0.007463\n",
            "-----------------------------------------------------------------\n",
            "AVERAGE AUC (Macro): 0.6054\n",
            "=================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1j3tnC3wPcztybcO9orhQAmKkn_rqXsag",
      "authorship_tag": "ABX9TyPzhdJztERUU97vujcJ13h6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}